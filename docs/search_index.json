[
["index.html", "SEM and R Chapter 1 SEM and R", " SEM and R Bill 2021-05-16 Chapter 1 SEM and R This is the starting point. "],
["intro.html", "Chapter 2 Introduction 2.1 Definitions (Basic Concepts) 2.2 The path diagram 2.3 Lavaan syntax 2.4 Regression and path analysis", " Chapter 2 Introduction The following R codes and texts are from UCLA website “https://stats.idre.ucla.edu/r/seminars/rsem/” and I do not own the copyright of the R codes or texts. I wrote this R Markdown file for my own study purpose. Given this consideration, please do NOT distribute this page in any way. 2.1 Definitions (Basic Concepts) 2.1.1 Observed variable Observed variable: A variable that exists in the data (a.k.a item or manifest variable) 2.1.2 Latent variable Latent variable: A variable that is constructed and does not exist in the data. 2.1.3 Exogenous variable Exogenous variable: An independent variable either observed (X) or latent (\\(\\xi\\)) that explains an engogenous variable. 2.1.4 Endogenous variable Endogenous variable: A dependent variable, either observed (Y) or latent (\\(\\eta\\)) that has a causal path leading to it. 2.1.5 Measurement model Measurement model: A model that links obseved variables with latent variables. 2.1.6 Indicator (in a measurement model) Indicator: An observed variable in a measurement model (can be exogenous or endogenous). 2.1.7 Factor Factor: A latent variable defined by its indicators (can be exogenous or endogeous). 2.1.8 Loading Loading: A path between an indicator and a factor. 2.1.9 Structural model Structural model: A model that specifies casual relationships among exogeous variables to endogeous variables (can be observed or latent). 2.1.10 Regerssion path Regression path: A path between exogeous and endogeous variables (can be observed or latent). 2.2 The path diagram Circles represent latent variables. Squares represent observed indicators. Triangles represent intercepts or means. One way arrows represent paths. Two-way arrows represent either variances or covariances. 2.3 Lavaan syntax \\(\\sim\\) predict: used for regression of observed outcome to observed predictors (e.g., \\(y \\sim x\\)). \\(= \\sim\\) indicator: used for latent variable to observed indicator in factor analysis measurement models (e.g., \\(f= \\sim q+r+s\\)). \\(\\sim \\sim\\) covariance: (e.g., \\(x \\sim \\sim x\\)). \\(\\sim 1\\) intercept or mean: (e.g., \\(x \\sim 1\\) estimates the mean of variable \\(x\\)). \\(1*\\) fixes parameter or loading to one: (e.g., \\(f =\\sim 1*q\\)). \\(NA *\\) free parameter or loading: used to override default marker method (e.g., \\(f=\\sim NA * q\\)). \\(a*\\) lables the parameter ‘a’: used for model constraints (e.g., \\(f=\\sim a*q\\)). 2.4 Regression and path analysis \\[y_{1}=b_{0}+b_{1}x_{1}+\\epsilon_{1}\\] \\[y_{1}=\\alpha+\\gamma_{1} x_{1}+\\zeta_{1}\\] \\(x_{1}\\) single exogenous variable \\(y_{1}\\) single endogenous variable \\(b_{0}\\), \\(\\alpha_{1}\\) intercept of \\(y_{1}\\) (alpha) \\(b_{1}\\), \\(\\gamma_{1}\\) regression coefficient (gamma) \\(\\epsilon_{1}\\), \\(\\zeta_{1}\\) residual of \\(y_{1}\\) (epsilon, zeta) \\(\\phi\\) variance or covariance of the exogenous variable (phi) \\(\\psi\\) residual variance or covariance of the endogenous variable (psi) "],
["real-data-example-simple-linear-regression.html", "Chapter 3 Real data example (Simple linear regression) 3.1 Read the data into the R Studio environment.", " Chapter 3 Real data example (Simple linear regression) 3.1 Read the data into the R Studio environment. It also calcuates the covariance matrix among all the variables in the data. dat &lt;- read.csv(&quot;https://stats.idre.ucla.edu/wp-content/uploads/2021/02/worland5.csv&quot;) cov(dat) ## motiv harm stabi ppsych ses verbal read arith spell ## motiv 100 77 59 -25 25 32 53 60 59 ## harm 77 100 58 -25 26 25 42 44 45 ## stabi 59 58 100 -16 18 27 36 38 38 ## ppsych -25 -25 -16 100 -42 -40 -39 -24 -31 ## ses 25 26 18 -42 100 40 43 37 33 ## verbal 32 25 27 -40 40 100 56 49 48 ## read 53 42 36 -39 43 56 100 73 87 ## arith 60 44 38 -24 37 49 73 100 72 ## spell 59 45 38 -31 33 48 87 72 100 var(dat$motiv) ## [1] 100 In the following, we conduct a simple linear regression. \\[sample \\ variance-covariance \\ matrix \\hat{\\sum} = \\mathbf{S} \\] m1a &lt;- lm(read ~ motiv, data=dat) (fit1a &lt;-summary(m1a)) ## ## Call: ## lm(formula = read ~ motiv, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.0995 -6.1109 0.2342 5.2237 24.0183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.232e-07 3.796e-01 0.00 1 ## motiv 5.300e-01 3.800e-02 13.95 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.488 on 498 degrees of freedom ## Multiple R-squared: 0.2809, Adjusted R-squared: 0.2795 ## F-statistic: 194.5 on 1 and 498 DF, p-value: &lt; 2.2e-16 library(lavaan) ## Warning: package &#39;lavaan&#39; was built under R version 3.6.3 ## This is lavaan 0.6-8 ## lavaan is FREE software! Please report any bugs. #simple regression using lavaan m1b &lt;- &#39; # regressions read ~ 1+ motiv # variance (optional) motiv ~~ motiv &#39; fit1b &lt;- sem(m1b, data=dat) summary(fit1b) ## lavaan 0.6-8 ended normally after 14 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## read ~ ## motiv 0.530 0.038 13.975 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .read -0.000 0.379 -0.000 1.000 ## motiv 0.000 0.447 0.000 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## motiv 99.800 6.312 15.811 0.000 ## .read 71.766 4.539 15.811 0.000 "],
["real-data-example-multiple-linear-regression.html", "Chapter 4 Real data example (Multiple linear regression)", " Chapter 4 Real data example (Multiple linear regression) m2 &lt;- &#39; # regressions read ~ 1 + ppsych + motiv # covariance ppsych ~~ motiv &#39; fit2 &lt;- sem(m2, data=dat) summary(fit2) ## lavaan 0.6-8 ended normally after 34 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 9 ## ## Number of observations 500 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## read ~ ## ppsych -0.275 0.037 -7.385 0.000 ## motiv 0.461 0.037 12.404 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## ppsych ~~ ## motiv -24.950 4.601 -5.423 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .read 0.000 0.360 0.000 1.000 ## ppsych -0.000 0.447 -0.000 1.000 ## motiv 0.000 0.447 0.000 1.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .read 64.708 4.092 15.811 0.000 ## ppsych 99.800 6.312 15.811 0.000 ## motiv 99.800 6.312 15.811 0.000 "],
["bootstrapping.html", "Chapter 5 Bootstrapping 5.1 Warning 5.2 Introduction 5.3 Normal distribution, SD, SE 5.4 Sample function 5.5 Proportion 5.6 boot package 5.7 Concept of Percentile 5.8 Bootstrapping for correlation interval", " Chapter 5 Bootstrapping 5.1 Warning Warning: This page is for my own personal study purpose. Distribution is prohibited. 5.2 Introduction The following note is made when I was studying Bret Larget’s note posted online. http://pages.stat.wisc.edu/~larget/stat302/chap3.pdf He used the data from LOck5data as an example. library(Lock5Data) data(CommuteAtlanta) str(CommuteAtlanta) ## &#39;data.frame&#39;: 500 obs. of 5 variables: ## $ City : Factor w/ 1 level &quot;Atlanta&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Age : int 19 55 48 45 48 43 48 41 47 39 ... ## $ Distance: int 10 45 12 4 15 33 15 4 25 1 ... ## $ Time : int 15 60 45 10 30 60 45 10 25 15 ... ## $ Sex : Factor w/ 2 levels &quot;F&quot;,&quot;M&quot;: 2 2 2 1 1 2 2 1 2 1 ... time.mean = with(CommuteAtlanta, mean(Time)) time.mean ## [1] 29.11 Now, he sampled a (b X n) table. Note that, the Atlanta data has 500 row, as it has 500 observations (or, people). But, in the following new matrix, it is a (1000 times 500) table. Also, it should be noted that the logic of sample function in R. This webpage provides some insight into this function. Basically, the following R code randomly sample a bigger sample of (1000 times 500) from those 500 data points. After that, the matrix function put such (1000 times 500) data points into a matrix of (1000 times 500). B = 1000 n = nrow(CommuteAtlanta) boot.samples = matrix(sample(CommuteAtlanta$Time, size = B * n, replace = TRUE), B, n) Next, we need to calculate the mean for each row. Remember, we have 1000 rows. Note that, 1 in the apply function indicates that we calculate means on each row, whereas 2 indicates to each column. boot.statistics = apply(boot.samples, 1, mean) We can then plot all the means. require(ggplot2) ## Loading required package: ggplot2 ## Warning: package &#39;ggplot2&#39; was built under R version 3.6.3 ggplot(data.frame(meanTime = boot.statistics),aes(x=meanTime)) + geom_histogram(binwidth=0.25,aes(y=..density..)) + geom_density(color=&quot;red&quot;) time.se = sd(boot.statistics) time.se ## [1] 0.9409072 me = ceiling(10 * 2 * time.se)/10 me ## [1] 1.9 round(time.mean, 1) + c(-1, 1) * me ## [1] 27.2 31.0 5.3 Normal distribution, SD, SE Note, if we do not use bootstraping, we can use the standard CI formula (https://www.mathsisfun.com/data/confidence-interval.html). This formula assumes normal distribution. As we can see, this is close to the result based on the bootstrapping method. \\[\\overline{X} \\pm Z \\frac{S}{\\sqrt{n}}=29.11 \\pm 1.96 \\frac{20.72}{\\sqrt{500}}=27.29, 30.93\\] Note that, in the following, the author used 2 times SE to calculate the CI. The relationship between SD and SE: “Now the sample mean will vary from sample to sample; the way this variation occurs is described by the “sampling distribution” of the mean. We can estimate how much sample means will vary from the standard deviation of this sampling distribution, which we call the standard error (SE) of the estimate of the mean. As the standard error is a type of standard deviation, confusion is understandable. Another way of considering the standard error is as a measure of the precision of the sample mean.\" (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1255808/) boot.mean = function(x,B,binwidth=NULL) { n = length(x) boot.samples = matrix( sample(x,size=n*B,replace=TRUE), B, n) boot.statistics = apply(boot.samples,1,mean) se = sd(boot.statistics) require(ggplot2) if ( is.null(binwidth) ) binwidth = diff(range(boot.statistics))/30 p = ggplot(data.frame(x=boot.statistics),aes(x=x)) + geom_histogram(aes(y=..density..),binwidth=binwidth) + geom_density(color=&quot;red&quot;) plot(p) interval = mean(x) + c(-1,1)*2*se print( interval ) return( list(boot.statistics = boot.statistics, interval=interval, se=se, plot=p) ) } out = with(CommuteAtlanta, boot.mean(Distance, B = 1000)) ## [1] 16.93922 19.37278 5.4 Sample function To understand the function of sample in R. sample(20,replace = TRUE) ## [1] 2 10 10 15 7 15 17 8 11 15 1 11 6 13 9 3 7 13 7 10 The following uses loop to do the resampling. It uses sample function to index the numbers that they want to sample from the original sample. That is, [] suggests the indexing. n = length(CommuteAtlanta$Distance) B = 1000 result = rep(NA, B) for (i in 1:B) { boot.sample = sample(n, replace = TRUE) result[i] = mean(CommuteAtlanta$Distance[boot.sample]) } with(CommuteAtlanta, mean(Distance) + c(-1, 1) * 2 * sd(result)) ## [1] 16.98167 19.33033 5.5 Proportion So far, we have dealed with means. How about porpotions?Remember that, when calculating means, it starts with a single column of data to calculate the mean. Similarly, when calculating porpotions, you can just use a single column of data. reeses = c(rep(1, 11), rep(0, 19)) reeses.boot = boot.mean(reeses, 1000, binwidth = 1/30) ## [1] 0.1905222 0.5428111 However, if we have 48 students (i.e., 48 observations) and thus we have a bigger sample. However, how can we do re-sampling? Based on the note, it is kind of simple. They group them together and then resample from it. Note that, when they re-sampling, the programming do not distinguish the difference between 48 observations. But just combined them as a single column (741+699=1440), and then generate a very long column (1440 times 1000) and then reshape it into a matrix (1440 time 1000). This is the basic logic of the boot.mean function. reeses = c(rep(1, 741), rep(0, 699)) reeses.boot = boot.mean(reeses, 1000, binwidth = 0.005) ## [1] 0.4890827 0.5400839 5.6 boot package After having a basic idea of boostrapping, we can then use the package of boot. library(boot) ## Warning: package &#39;boot&#39; was built under R version 3.6.3 data(CommuteAtlanta) my.mean = function(x, indices) { return( mean( x[indices] ) ) } time.boot = boot(CommuteAtlanta$Time, my.mean, 10000) boot.ci(time.boot) ## Warning in boot.ci(time.boot): bootstrap variances needed for studentized ## intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = time.boot) ## ## Intervals : ## Level Normal Basic ## 95% (27.29, 30.93 ) (27.23, 30.87 ) ## ## Level Percentile BCa ## 95% (27.35, 30.99 ) (27.44, 31.12 ) ## Calculations and Intervals on Original Scale 5.7 Concept of Percentile require(Lock5Data) data(ImmuneTea) tea = with(ImmuneTea, InterferonGamma[Drink==&quot;Tea&quot;]) coffee = with(ImmuneTea, InterferonGamma[Drink==&quot;Coffee&quot;]) tea.mean = mean(tea) coffee.mean = mean(coffee) tea.n = length(tea) coffee.n = length(coffee) B = 500 # create empty arrays for the means of each sample tea.boot = numeric(B) coffee.boot = numeric(B) # Use a for loop to take the samples for ( i in 1:B ) { tea.boot[i] = mean(sample(tea,size=tea.n,replace=TRUE)) coffee.boot[i] = mean(sample(coffee,size=coffee.n,replace=TRUE)) } boot.stat = tea.boot - coffee.boot boot.stat ## [1] 12.9636364 22.3000000 6.8272727 13.4909091 14.2363636 ## [6] 11.2545455 11.4818182 14.0818182 23.4272727 20.9727273 ## [11] 28.1090909 17.1818182 22.4454545 14.6363636 14.6636364 ## [16] 1.3454545 10.8181818 12.9090909 30.2545455 28.9181818 ## [21] 14.7363636 13.6909091 -1.0090909 21.9545455 22.9909091 ## [26] 20.8000000 7.0454545 35.7727273 1.7090909 11.7363636 ## [31] 27.1454545 8.5181818 26.7545455 19.4363636 11.3181818 ## [36] 20.6909091 11.6727273 13.0363636 8.3818182 26.4000000 ## [41] 7.2818182 16.6090909 18.8090909 12.2727273 15.8818182 ## [46] 21.0454545 2.8909091 20.5181818 14.1090909 15.2454545 ## [51] 17.2909091 16.8181818 3.4363636 14.9181818 29.0545455 ## [56] 13.0363636 30.0090909 8.6545455 2.0818182 16.8909091 ## [61] 27.1818182 35.0090909 17.0909091 21.4909091 16.7000000 ## [66] 19.9818182 5.4363636 -2.1727273 24.3090909 29.1545455 ## [71] 29.1363636 12.8545455 16.7545455 16.7818182 20.7818182 ## [76] 25.7636364 18.0636364 25.3272727 19.8818182 13.3454545 ## [81] 15.7000000 19.3181818 2.7545455 13.2272727 22.4727273 ## [86] 16.9454545 25.3727273 4.7636364 23.9454545 22.4727273 ## [91] 20.4636364 27.4909091 15.9636364 7.2272727 2.1454545 ## [96] 18.0000000 6.2909091 17.4000000 23.3090909 15.7272727 ## [101] 10.1454545 7.7272727 18.3000000 11.7363636 26.3454545 ## [106] 24.6818182 17.4363636 23.0181818 32.7909091 4.8818182 ## [111] 23.9636364 0.8272727 20.0545455 15.6636364 14.5454545 ## [116] 12.0818182 21.2545455 10.6545455 10.3090909 18.9545455 ## [121] 3.9545455 14.7545455 31.0181818 15.0363636 12.2090909 ## [126] 17.0000000 11.7636364 20.1909091 13.3000000 13.5272727 ## [131] 7.7636364 15.2272727 12.8363636 23.4818182 14.3181818 ## [136] 21.9909091 16.2000000 14.9636364 28.8909091 39.7727273 ## [141] 17.9545455 5.4272727 24.9545455 22.3636364 16.3454545 ## [146] 9.9909091 12.0272727 9.9363636 23.3636364 6.2636364 ## [151] 12.0090909 19.4818182 9.1272727 9.5636364 26.6545455 ## [156] 15.5454545 19.7363636 23.2727273 20.2272727 22.7090909 ## [161] 16.8090909 22.2818182 24.8818182 14.4727273 3.5363636 ## [166] 30.0545455 21.9363636 16.6545455 19.8181818 18.3363636 ## [171] 6.4909091 6.4818182 22.2818182 9.3181818 28.8272727 ## [176] 20.2909091 19.7545455 8.7909091 24.3545455 31.1363636 ## [181] 15.4272727 4.5363636 22.0363636 20.1818182 12.5272727 ## [186] 11.7818182 23.1000000 18.2363636 20.6545455 17.4454545 ## [191] 33.5454545 17.3181818 22.8545455 13.0818182 10.5363636 ## [196] 14.4909091 16.1727273 9.8363636 13.8818182 3.3090909 ## [201] 8.6818182 24.9636364 21.8636364 27.9363636 25.6909091 ## [206] 21.0818182 24.9636364 27.6090909 8.0363636 8.0454545 ## [211] 20.3272727 1.1909091 11.2909091 16.4181818 25.5000000 ## [216] 16.9272727 18.3272727 17.2909091 13.7636364 11.2818182 ## [221] 16.2727273 4.5909091 12.9727273 24.8454545 14.3272727 ## [226] 4.4727273 34.7181818 7.3272727 26.3090909 13.0727273 ## [231] 19.8636364 15.5363636 23.7545455 27.3727273 22.1636364 ## [236] 21.8000000 13.3363636 19.4818182 16.6090909 12.8909091 ## [241] 2.9636364 32.4181818 -10.1181818 26.1090909 13.9090909 ## [246] 6.7909091 10.9090909 19.2454545 17.5181818 9.1636364 ## [251] 0.2545455 20.9181818 9.7090909 15.0363636 15.3818182 ## [256] 17.9363636 12.2181818 13.2636364 26.3454545 6.6909091 ## [261] 14.4363636 12.9636364 20.6000000 0.8636364 5.0909091 ## [266] 25.1818182 2.3727273 30.5272727 3.9272727 34.3818182 ## [271] 6.4272727 28.4636364 24.8454545 16.3636364 18.9272727 ## [276] 16.6090909 2.7181818 28.8454545 18.9272727 4.5181818 ## [281] 11.7545455 17.4636364 21.4363636 20.0181818 13.6727273 ## [286] 19.5818182 1.8818182 29.5909091 25.1363636 20.2727273 ## [291] 24.6363636 19.4363636 22.7818182 23.1090909 25.5727273 ## [296] 13.5636364 15.6363636 20.5181818 24.2636364 9.4000000 ## [301] 23.6272727 9.7636364 29.7636364 4.6181818 18.3363636 ## [306] 8.5818182 11.2454545 24.6181818 17.6000000 11.2818182 ## [311] 1.6272727 25.8818182 21.7636364 32.9272727 11.3090909 ## [316] 16.5181818 17.1636364 17.0818182 10.5545455 21.0181818 ## [321] 26.3909091 19.1000000 20.3909091 11.6636364 6.4181818 ## [326] 12.9636364 16.8909091 13.7818182 15.9363636 20.5545455 ## [331] 19.2545455 10.9454545 20.6363636 21.0545455 27.9454545 ## [336] 16.3000000 27.6272727 18.1636364 41.1090909 12.5181818 ## [341] 8.6818182 26.4727273 15.7454545 7.4818182 15.9181818 ## [346] 26.7727273 17.3181818 17.3272727 19.1545455 13.3363636 ## [351] 25.3636364 16.4454545 11.6545455 23.9000000 16.2636364 ## [356] 21.6636364 15.9090909 19.8727273 18.3363636 15.9909091 ## [361] 12.9272727 12.1909091 6.6727273 20.3454545 27.7818182 ## [366] 26.9727273 24.2000000 2.4272727 17.5636364 22.4181818 ## [371] 17.9909091 29.8545455 22.6909091 29.8272727 23.5454545 ## [376] 16.2727273 12.3636364 14.0818182 20.3181818 31.6000000 ## [381] 19.5090909 26.3727273 20.8454545 12.1909091 17.0090909 ## [386] 12.2454545 18.4818182 27.8000000 24.1363636 16.6272727 ## [391] 23.6727273 21.6909091 -0.2636364 24.6727273 35.4545455 ## [396] 17.5000000 2.9000000 24.2454545 31.1727273 12.0636364 ## [401] 27.9363636 20.5000000 0.9545455 15.7727273 17.0090909 ## [406] 22.8090909 15.1727273 16.3181818 15.2090909 15.0090909 ## [411] 14.9909091 7.2454545 16.3363636 17.7545455 23.5272727 ## [416] 16.4000000 22.7272727 21.2454545 25.9909091 24.7727273 ## [421] 25.1545455 14.3090909 22.0727273 19.8818182 6.8272727 ## [426] 25.7818182 30.4636364 11.1000000 7.0363636 24.6818182 ## [431] 21.1727273 14.5000000 26.3000000 28.1181818 17.2909091 ## [436] 11.9272727 24.2636364 13.4909091 9.4545455 21.1636364 ## [441] 20.9363636 11.6636364 25.6000000 20.7272727 23.4272727 ## [446] 4.2545455 22.9545455 6.3363636 14.1181818 23.8272727 ## [451] 21.5909091 21.3363636 15.7363636 23.6909091 16.1181818 ## [456] 11.8363636 11.6909091 18.1545455 9.9818182 25.2000000 ## [461] 17.7000000 19.0727273 16.0181818 14.4909091 5.5909091 ## [466] 11.6636364 15.2000000 21.9818182 5.5363636 16.5181818 ## [471] 15.4545455 11.8454545 25.7545455 13.3909091 11.8545455 ## [476] 11.4090909 8.6727273 29.4636364 17.4272727 13.2363636 ## [481] 15.9636364 17.8454545 25.4818182 23.0818182 6.9545455 ## [486] 9.7363636 17.2000000 16.1909091 19.7363636 22.8909091 ## [491] 3.4818182 23.1909091 19.5727273 8.1818182 21.4545455 ## [496] 23.6636364 24.3000000 13.2636364 20.3363636 11.5818182 # Find endpoints for 90%, 95%, and 99% bootstrap confidence intervals using percentiles. # 90%: 5% 95% quantile(boot.stat,c(0.05,0.95)) ## 5% 95% ## 3.533636 29.170000 # 95%: 2.5% 97.5% quantile(boot.stat,c(0.025,0.975)) ## 2.5% 97.5% ## 1.976818 31.155455 # 99%: 0.5% 99.5% quantile(boot.stat,c(0.005,0.995)) ## 0.5% 99.5% ## -0.6400909 35.6152273 5.8 Bootstrapping for correlation interval Some data and code are from: https://blog.methodsconsultants.com/posts/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/ data_correlation&lt;-read.csv(&quot;data_correlation.csv&quot;,fileEncoding=&quot;UTF-8-BOM&quot;) data_correlation ## Student LSAT GPA ## 1 1 576 3.39 ## 2 2 635 3.30 ## 3 3 558 2.81 ## 4 4 578 3.03 ## 5 5 666 3.44 ## 6 6 580 3.07 ## 7 7 555 3.00 ## 8 8 661 3.43 ## 9 9 651 3.36 ## 10 10 605 3.13 ## 11 11 653 3.12 ## 12 12 575 2.74 ## 13 13 545 2.76 ## 14 14 572 2.88 ## 15 15 594 2.96 cor.test(data_correlation$LSAT,data_correlation$GPA) ## ## Pearson&#39;s product-moment correlation ## ## data: data_correlation$LSAT and data_correlation$GPA ## t = 4.4413, df = 13, p-value = 0.0006651 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4385108 0.9219648 ## sample estimates: ## cor ## 0.7763745 In the following, I will write my own code to execute the bootstrapping. I set the bootstrapping number only 500, for illustrative purposes. As we can see, the distribution is not symmetrical. As we can see, the quantile result and c(-1, 1) X 2 are not the same, as the latter assumes symmetrical distribution. However, based on the histogram, we know it is not the case. Thus, quantile would be more appropriate. You can compare the result with that from the boot function. n_row = nrow(data_correlation) n_row ## [1] 15 set.seed(12345) B = 500 result = rep(NA, B) for (i in 1:B) { boot.sample = sample(n_row, replace = TRUE) result_temp = cor.test(data_correlation[boot.sample,]$LSAT,data_correlation[boot.sample,]$GPA) result[i]=result_temp$estimate } hist(result) # 95%: 2.5% 97.5% quantile(result,c(0.025,0.975)) ## 2.5% 97.5% ## 0.4369293 0.9556859 sd(result) ## [1] 0.1342631 mean(result) + c(-1, 1) * 1.96 * sd(result) ## [1] 0.5107704 1.0370816 cor(data_correlation$LSAT,data_correlation$GPA) ## [1] 0.7763745 cor(data_correlation$LSAT,data_correlation$GPA)+ c(-1, 1) * 1.96 * sd(result) ## [1] 0.5132189 1.0395301 # why add 0.005? Not sure. The following is from the webpage. Later note: please refer to the webpage, as it provides the logic of basic interval. 0.776+0.005+c(-1, 1) * 1.96 * 0.131 ## [1] 0.52424 1.03776 In the blog mentioned above, the author used the boot function in R. For the logic of basic interval, please refer to: https://blog.methodsconsultants.com/posts/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/ library(boot) get_r &lt;- function(data, indices, x, y) { d &lt;- data[indices, ] r &lt;- round(as.numeric(cor(d[x], d[y])), 3) r} set.seed(12345) boot_out &lt;- boot( data_correlation, x = &quot;LSAT&quot;, y = &quot;GPA&quot;, R = 500, statistic = get_r ) boot.ci(boot_out) ## Warning in boot.ci(boot_out): bootstrap variances needed for studentized ## intervals ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 500 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot_out) ## ## Intervals : ## Level Normal Basic ## 95% ( 0.5247, 1.0368 ) ( 0.5900, 1.0911 ) ## ## Level Percentile BCa ## 95% ( 0.4609, 0.9620 ) ( 0.3948, 0.9443 ) ## Calculations and Intervals on Original Scale ## Some BCa intervals may be unstable "],
["poisson-regression.html", "Chapter 6 Poisson Regression 6.1 Basic idea 6.2 Trying to understand 6.3 Deviance 6.4 Overdispersion (using another example)", " Chapter 6 Poisson Regression 6.1 Basic idea The following is based on the lecture note of https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html There is also some R code related to this. https://rdrr.io/github/sta303-bolton/sta303w8/f/inst/rmarkdown/templates/philippines/skeleton/skeleton.Rmd data_HH &lt;- read.csv(&quot;https://raw.githubusercontent.com/proback/BeyondMLR/master/data/fHH1.csv&quot;) head(data_HH) ## X location age total numLT5 roof ## 1 1 CentralLuzon 65 0 0 Predominantly Strong Material ## 2 2 MetroManila 75 3 0 Predominantly Strong Material ## 3 3 DavaoRegion 54 4 0 Predominantly Strong Material ## 4 4 Visayas 49 3 0 Predominantly Strong Material ## 5 5 MetroManila 74 3 0 Predominantly Strong Material ## 6 6 Visayas 59 6 0 Predominantly Strong Material \\[log (\\lambda_X) =\\beta_0+\\beta_1 X\\] \\[log (\\lambda_{X+1}) =\\beta_0+\\beta_1 (X+1)\\] Thus, \\[log (\\lambda_{X+1})-log (\\lambda_X) =(\\beta_0+\\beta_1 (X+1))-(\\beta_0+\\beta_1 X)\\] Thus, \\[log (\\frac{\\lambda_{X+1}}{\\lambda_X}) =\\beta_1\\] Thus, \\[\\frac{\\lambda_{X+1}}{\\lambda_X} =e^{\\beta_1}\\] Note that, \\(\\lambda\\) here is the mean. It is poisson regression, and the parameter is the mean. Thus, \\(\\frac{\\lambda_{X+1}}{\\lambda_X} =e^{\\beta_1}\\) suggests the ratio change in the DV as the IV change in one unit. \\[log (\\hat{\\lambda}) =b_0+b_1 Age\\] result_1 = glm(total ~ age, family = poisson, data = data_HH) summary(result_1) ## ## Call: ## glm(formula = total ~ age, family = poisson, data = data_HH) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9079 -0.9637 -0.2155 0.6092 4.9561 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.5499422 0.0502754 30.829 &lt; 2e-16 *** ## age -0.0047059 0.0009363 -5.026 5.01e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 2362.5 on 1499 degrees of freedom ## Residual deviance: 2337.1 on 1498 degrees of freedom ## AIC: 6714 ## ## Number of Fisher Scoring iterations: 5 \\[\\frac{\\lambda_{Age+1}}{\\lambda_{Age}} =e^{\\beta_1}=e^{-0.0047}=0.995\\] But, what does it mean? It is a bit tricky. But, we can make some modification to help us understand. \\[\\lambda_{Age+1} =0.995 \\lambda_{Age}\\] \\[\\lambda_{Age+1} - \\lambda_{Age}=0.995 \\lambda_{Age}- \\lambda_{Age}=-0.005 \\lambda_{Age}\\] Thus, we can understand that, the difference in the household size mean by changing 1 unit of age (i.e., \\(\\lambda_{Age+1} - \\lambda_{Age}\\)) is \\(-0.005 \\lambda_{Age}\\). That is, the difference in the household size mean by changing 1 unit of age (i.e., \\(\\lambda_{Age+1} - \\lambda_{Age}\\)) is a decrease of 5% of \\(\\lambda_{Age}\\). We can then calculate the confidence interval. \\[(\\hat{\\beta_1}-Z*SE(\\hat{\\beta_1}), \\hat{\\beta_1}+Z*SE(\\hat{\\beta_1}))\\] \\[(-0.0047-1.96*0.00094,-0.0047+1.96*0.00094)=(−0.0065,−0.0029)\\] We can then plug them back to the exponential. exp(−0.0065) ## [1] 0.9935211 exp(−0.0029) ## [1] 0.9971042 \\[(e^{−0.0065},e^{−0.0029})=(0.9935,0.9971)\\] You can also get the confidence interval directly use R code confint(result_1) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 1.451170100 1.648249185 ## age -0.006543163 -0.002872717 exp(confint(result_1)) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 4.2681057 5.1978713 ## age 0.9934782 0.9971314 Note that, we use original beta to construct a confidence interval and then exponentiate the endpoints is due to the fact that the oringal one is more close to normal distribution. 6.2 Trying to understand With \\(\\hat{\\beta_0} = 1.55\\) and \\(\\hat{\\beta_1}=-0.005\\), we can write down the following. I also simulated the data and showed the relationship between X and Y. As we can see the figure, the relationship is pretty linear. Thus, something to keep in mind, the poisson distribution we typically see is the histogram of Y, rather than the relationship between X and Y. \\[log(\\hat{\\lambda})=1.55-0.005 Age\\] \\[\\hat{\\lambda}=e^{1.55-0.005 Age}\\] data_age&lt;-seq(10,100,0.5) f_age&lt;-function(x){exp(1.55-(0.005*x))} cbind(data_age,f_age(data_age)) ## data_age ## [1,] 10.0 4.481689 ## [2,] 10.5 4.470499 ## [3,] 11.0 4.459337 ## [4,] 11.5 4.448202 ## [5,] 12.0 4.437096 ## [6,] 12.5 4.426017 ## [7,] 13.0 4.414965 ## [8,] 13.5 4.403942 ## [9,] 14.0 4.392946 ## [10,] 14.5 4.381977 ## [11,] 15.0 4.371036 ## [12,] 15.5 4.360122 ## [13,] 16.0 4.349235 ## [14,] 16.5 4.338376 ## [15,] 17.0 4.327543 ## [16,] 17.5 4.316738 ## [17,] 18.0 4.305960 ## [18,] 18.5 4.295208 ## [19,] 19.0 4.284483 ## [20,] 19.5 4.273786 ## [21,] 20.0 4.263115 ## [22,] 20.5 4.252470 ## [23,] 21.0 4.241852 ## [24,] 21.5 4.231261 ## [25,] 22.0 4.220696 ## [26,] 22.5 4.210157 ## [27,] 23.0 4.199645 ## [28,] 23.5 4.189159 ## [29,] 24.0 4.178699 ## [30,] 24.5 4.168265 ## [31,] 25.0 4.157858 ## [32,] 25.5 4.147476 ## [33,] 26.0 4.137120 ## [34,] 26.5 4.126791 ## [35,] 27.0 4.116486 ## [36,] 27.5 4.106208 ## [37,] 28.0 4.095955 ## [38,] 28.5 4.085728 ## [39,] 29.0 4.075527 ## [40,] 29.5 4.065351 ## [41,] 30.0 4.055200 ## [42,] 30.5 4.045075 ## [43,] 31.0 4.034975 ## [44,] 31.5 4.024900 ## [45,] 32.0 4.014850 ## [46,] 32.5 4.004825 ## [47,] 33.0 3.994826 ## [48,] 33.5 3.984851 ## [49,] 34.0 3.974902 ## [50,] 34.5 3.964977 ## [51,] 35.0 3.955077 ## [52,] 35.5 3.945201 ## [53,] 36.0 3.935351 ## [54,] 36.5 3.925525 ## [55,] 37.0 3.915723 ## [56,] 37.5 3.905946 ## [57,] 38.0 3.896193 ## [58,] 38.5 3.886465 ## [59,] 39.0 3.876761 ## [60,] 39.5 3.867081 ## [61,] 40.0 3.857426 ## [62,] 40.5 3.847794 ## [63,] 41.0 3.838187 ## [64,] 41.5 3.828603 ## [65,] 42.0 3.819044 ## [66,] 42.5 3.809508 ## [67,] 43.0 3.799996 ## [68,] 43.5 3.790508 ## [69,] 44.0 3.781043 ## [70,] 44.5 3.771603 ## [71,] 45.0 3.762185 ## [72,] 45.5 3.752792 ## [73,] 46.0 3.743421 ## [74,] 46.5 3.734075 ## [75,] 47.0 3.724751 ## [76,] 47.5 3.715451 ## [77,] 48.0 3.706174 ## [78,] 48.5 3.696920 ## [79,] 49.0 3.687689 ## [80,] 49.5 3.678481 ## [81,] 50.0 3.669297 ## [82,] 50.5 3.660135 ## [83,] 51.0 3.650996 ## [84,] 51.5 3.641880 ## [85,] 52.0 3.632787 ## [86,] 52.5 3.623716 ## [87,] 53.0 3.614668 ## [88,] 53.5 3.605643 ## [89,] 54.0 3.596640 ## [90,] 54.5 3.587659 ## [91,] 55.0 3.578701 ## [92,] 55.5 3.569766 ## [93,] 56.0 3.560853 ## [94,] 56.5 3.551962 ## [95,] 57.0 3.543093 ## [96,] 57.5 3.534246 ## [97,] 58.0 3.525421 ## [98,] 58.5 3.516619 ## [99,] 59.0 3.507838 ## [100,] 59.5 3.499080 ## [101,] 60.0 3.490343 ## [102,] 60.5 3.481628 ## [103,] 61.0 3.472935 ## [104,] 61.5 3.464263 ## [105,] 62.0 3.455613 ## [106,] 62.5 3.446985 ## [107,] 63.0 3.438379 ## [108,] 63.5 3.429793 ## [109,] 64.0 3.421230 ## [110,] 64.5 3.412687 ## [111,] 65.0 3.404166 ## [112,] 65.5 3.395666 ## [113,] 66.0 3.387188 ## [114,] 66.5 3.378730 ## [115,] 67.0 3.370294 ## [116,] 67.5 3.361879 ## [117,] 68.0 3.353485 ## [118,] 68.5 3.345111 ## [119,] 69.0 3.336759 ## [120,] 69.5 3.328428 ## [121,] 70.0 3.320117 ## [122,] 70.5 3.311827 ## [123,] 71.0 3.303558 ## [124,] 71.5 3.295309 ## [125,] 72.0 3.287081 ## [126,] 72.5 3.278874 ## [127,] 73.0 3.270687 ## [128,] 73.5 3.262520 ## [129,] 74.0 3.254374 ## [130,] 74.5 3.246248 ## [131,] 75.0 3.238143 ## [132,] 75.5 3.230058 ## [133,] 76.0 3.221993 ## [134,] 76.5 3.213948 ## [135,] 77.0 3.205923 ## [136,] 77.5 3.197918 ## [137,] 78.0 3.189933 ## [138,] 78.5 3.181968 ## [139,] 79.0 3.174023 ## [140,] 79.5 3.166098 ## [141,] 80.0 3.158193 ## [142,] 80.5 3.150307 ## [143,] 81.0 3.142441 ## [144,] 81.5 3.134595 ## [145,] 82.0 3.126768 ## [146,] 82.5 3.118961 ## [147,] 83.0 3.111174 ## [148,] 83.5 3.103405 ## [149,] 84.0 3.095657 ## [150,] 84.5 3.087927 ## [151,] 85.0 3.080217 ## [152,] 85.5 3.072526 ## [153,] 86.0 3.064854 ## [154,] 86.5 3.057202 ## [155,] 87.0 3.049568 ## [156,] 87.5 3.041954 ## [157,] 88.0 3.034358 ## [158,] 88.5 3.026782 ## [159,] 89.0 3.019224 ## [160,] 89.5 3.011686 ## [161,] 90.0 3.004166 ## [162,] 90.5 2.996665 ## [163,] 91.0 2.989183 ## [164,] 91.5 2.981719 ## [165,] 92.0 2.974274 ## [166,] 92.5 2.966848 ## [167,] 93.0 2.959440 ## [168,] 93.5 2.952050 ## [169,] 94.0 2.944680 ## [170,] 94.5 2.937327 ## [171,] 95.0 2.929993 ## [172,] 95.5 2.922677 ## [173,] 96.0 2.915379 ## [174,] 96.5 2.908100 ## [175,] 97.0 2.900839 ## [176,] 97.5 2.893596 ## [177,] 98.0 2.886371 ## [178,] 98.5 2.879164 ## [179,] 99.0 2.871975 ## [180,] 99.5 2.864804 ## [181,] 100.0 2.857651 plot(data_age,f_age(data_age)) hist(f_age(data_age)) 6.3 Deviance basic_model &lt;- glm(total ~ 1, family = poisson, data = data_HH) deviance_1 &lt;- anova(basic_model, result_1, test = &quot;Chisq&quot;) deviance_1 ## Analysis of Deviance Table ## ## Model 1: total ~ 1 ## Model 2: total ~ age ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 1499 2362.5 ## 2 1498 2337.1 1 25.399 4.661e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 6.4 Overdispersion (using another example) data_edu&lt;-read.csv(&quot;https://raw.githubusercontent.com/proback/BeyondMLR/master/data/c_data.csv&quot;) head(data_edu) ## Enrollment type nv nvrate enroll1000 region ## 1 5590 U 30 5.36672630 5.590 SE ## 2 540 C 0 0.00000000 0.540 SE ## 3 35747 U 23 0.64341064 35.747 W ## 4 28176 C 1 0.03549120 28.176 W ## 5 10568 U 1 0.09462528 10.568 SW ## 6 3127 U 0 0.00000000 3.127 SW hist(data_edu$nv) results_3&lt;- glm(nv ~ type + region, family = poisson, offset = log(enroll1000), data = data_edu) summary(results_3) ## ## Call: ## glm(formula = nv ~ type + region, family = poisson, data = data_edu, ## offset = log(enroll1000)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.5697 -1.9079 -0.7233 0.8738 8.4564 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.60161 0.17120 -9.355 &lt; 2e-16 *** ## typeU 0.34011 0.13234 2.570 0.01017 * ## regionMW 0.09942 0.17752 0.560 0.57547 ## regionNE 0.78109 0.15305 5.103 3.33e-07 *** ## regionSE 0.87668 0.15314 5.725 1.04e-08 *** ## regionSW 0.50251 0.18508 2.715 0.00663 ** ## regionW 0.27324 0.18741 1.458 0.14484 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 491.00 on 80 degrees of freedom ## Residual deviance: 426.01 on 74 degrees of freedom ## AIC: 657.89 ## ## Number of Fisher Scoring iterations: 6 results_4 &lt;- glm(nv ~ type + region, family = quasipoisson, offset = log(enroll1000), data = data_edu) summary(results_4) ## ## Call: ## glm(formula = nv ~ type + region, family = quasipoisson, data = data_edu, ## offset = log(enroll1000)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.5697 -1.9079 -0.7233 0.8738 8.4564 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.60161 0.48312 -3.315 0.00142 ** ## typeU 0.34011 0.37347 0.911 0.36542 ## regionMW 0.09942 0.50097 0.198 0.84324 ## regionNE 0.78109 0.43191 1.808 0.07460 . ## regionSE 0.87668 0.43216 2.029 0.04609 * ## regionSW 0.50251 0.52230 0.962 0.33913 ## regionW 0.27324 0.52887 0.517 0.60694 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 7.963687) ## ## Null deviance: 491.00 on 80 degrees of freedom ## Residual deviance: 426.01 on 74 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 6 difference_dev &lt;- anova(results_4, results_3, test = &quot;F&quot;) difference_dev ## Analysis of Deviance Table ## ## Model 1: nv ~ type + region ## Model 2: nv ~ type + region ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 74 426.01 ## 2 74 426.01 0 0 "],
["use-r-for-mediation.html", "Chapter 7 Use R for mediation 7.1 Normal Distribution Case 7.2 Poisson Distribution Case", " Chapter 7 Use R for mediation References: https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html https://advstats.psychstat.org/book/mediation/index.php 7.1 Normal Distribution Case The following code generates the data to be used in the mediation model. Based on the histogram, we can see that it follows Poisson distribution. # Generate data for mediation analysis # https://ademos.people.uic.edu/Chapter14.html # Generate Possion data # https://stats.stackexchange.com/questions/27443/generate-data-samples-from-poisson-regression set.seed(123) N &lt;- 200 X &lt;- rnorm(N, 1, 1) M &lt;- 0.6*X + rnorm(N, 0, 1) mu &lt;- exp(0.2*X+0.8*M ) Y &lt;- rpois(n=N, lambda=mu) Test_data &lt;- data.frame(X, M, Y) head(Test_data) ## X M Y ## 1 0.4395244 2.4625250 7 ## 2 0.7698225 1.7743065 4 ## 3 2.5587083 1.2700799 2 ## 4 1.0705084 1.1854991 1 ## 5 1.1292877 0.2632327 1 ## 6 2.7150650 1.1527921 9 hist(Test_data$Y) # write.csv(Test_data,&quot;Test_data.csv&quot;) Next, while in reality it follows Poisson distribution, the following assumes normal distribution. You will find the results are consistent with PROCESS. Normal_Mediation&lt;-function(X, M, Y, data_used, resampling_size=5000) { result = rep(NA, resampling_size) n_row = nrow(data_used) for (i in 1:resampling_size) { boot.sample = sample(n_row, replace = TRUE) data_temp&lt;-data_used[boot.sample,] # a path result_a_temp&lt;-lm(M~X, data = data_temp)$coefficients names(result_a_temp) &lt;- NULL a_0_temp&lt;-result_a_temp[1] a_1_temp&lt;-result_a_temp[2] # b path result_b_temp&lt;-lm(Y~M+X, data = data_temp)$coefficients names(result_b_temp) &lt;- NULL b_0_temp&lt;-result_b_temp[1] b_1_temp&lt;-result_b_temp[2] c_1_apostrophe_temp&lt;-result_b_temp[3] #calculating the indirect effect indirect_temp&lt;-a_1_temp*b_1_temp result[i]=indirect_temp } hist(result) sd(result) print(mean(result) + c(-1, 1) * 2 * sd(result)) print(quantile(result,c(0.025,0.975))) } Normal_Mediation(X=X, M=M,Y=Y, data_used = Test_data,resampling_size=5000) ## [1] 0.9502475 2.4221161 ## 2.5% 97.5% ## 1.021319 2.453762 7.2 Poisson Distribution Case However, the problem is that the DV is count data. So, it is better to take that into consideration. The following is based on the paper of Geldhof 2017, Accommodating binary and count variables in mediation, A case for conditional indirect effects. In particular, Poisson regression uses the log link. For the b path function, it is as follows. \\[log(Y)=e^{b_0+b_1M+c^{&#39;}X}\\] Thus, its first partial derivative again M is as follows. \\[b_1e^{b_0+b_1M+c^{&#39;}X}\\] Where, \\[M=a_0+a_1X\\] Thus, the indirect effect is as follows. \\[IndirectEffect = a_1b_1e^{b_0+b_1M+c^{&#39;}X}=a_1b_1e^{b_0+b_1(a_0+a_1X)+c^{&#39;}X}\\] As we can see the indirect effect is not a constant, as it depends on X. Different X values will lead to different indirect effects. Thus, you can see the following R code takes this into consideration. # x_predetermined = 0 : X = Mean # x_predetermined = 1 : X = Mean + SD # x_predetermined = -1 : X = Mean - SD Poisson_Mediation&lt;-function(X, M, Y, data_used, x_predetermined=0, resampling_size=5000) { result = rep(NA, resampling_size) n_row = nrow(data_used) if(x_predetermined==0){x_predetermined=mean(data_used$X)} else if (x_predetermined==-1){x_predetermined=mean(data_used$X)-sd(data_used$X)} else(x_predetermined=mean(data_used$X)+sd(data_used$X)) for (i in 1:resampling_size) { boot.sample = sample(n_row, replace = TRUE) data_temp&lt;-data_used[boot.sample,] # a path result_a_temp&lt;-lm(M~X, data = data_temp)$coefficients names(result_a_temp) &lt;- NULL a_0_temp&lt;-result_a_temp[1] a_1_temp&lt;-result_a_temp[2] # b path result_b_temp&lt;-glm(Y~M+X, data = data_temp, family = quasipoisson)$coefficients names(result_b_temp) &lt;- NULL b_0_temp&lt;-result_b_temp[1] b_1_temp&lt;-result_b_temp[2] c_1_apostrophe_temp&lt;-result_b_temp[3] #calculating the indirect effect M_estimated_temp=a_0_temp+a_1_temp*x_predetermined indirect_temp&lt;-a_1_temp*b_1_temp*exp(b_0_temp+b_1_temp*M_estimated_temp+c_1_apostrophe_temp*x_predetermined) result[i]=indirect_temp } hist(result) quantile(result,c(0.025,0.975)) } # X = Mean Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=0,resampling_size=5000) ## 2.5% 97.5% ## 0.6432712 1.2018952 # X = Mean - 1 SD Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=-1,resampling_size=5000) ## 2.5% 97.5% ## 0.3773349 0.6186202 # X = Mean + 1 SD Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=1,resampling_size=5000) ## 2.5% 97.5% ## 1.073396 2.435226 "]
]
