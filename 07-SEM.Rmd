# Bootstrapping 

## Warning 

__Warning:__

__This page is for my own personal study purpose. Distribution is prohibited.__

---

## Introduction

The following note is made when I was studying Bret Larget's note posted online.
http://pages.stat.wisc.edu/~larget/stat302/chap3.pdf

He used the data from LOck5data as an example.

```{r}
library(Lock5Data)
data(CommuteAtlanta)
str(CommuteAtlanta)

time.mean = with(CommuteAtlanta, mean(Time))

time.mean
```

Now, he sampled a (b X n) table. Note that, the Atlanta data has 500 row, as it has 500 observations (or, people). But, in the following new matrix, it is a (1000 times 500) table. Also, it should be noted that the logic of sample function in R. This webpage provides some insight into this function. Basically, the following R code randomly sample a bigger sample of (1000 times 500) from those 500 data points. After that, the matrix function put such (1000 times 500) data points into a matrix of (1000 times 500).  


```{r}
B = 1000
n = nrow(CommuteAtlanta)
boot.samples = matrix(sample(CommuteAtlanta$Time, size = B * n, replace = TRUE),
                      B, n)
```

Next, we need to calculate the mean for each row. Remember, we have 1000 rows. Note that, 1 in the apply function indicates that we calculate means on each row, whereas 2 indicates to each column. 

```{R}
boot.statistics = apply(boot.samples, 1, mean)

```

We can then plot all the means.

```{R}
require(ggplot2)
ggplot(data.frame(meanTime = boot.statistics),aes(x=meanTime)) +
geom_histogram(binwidth=0.25,aes(y=..density..)) +
geom_density(color="red")
```



```{r}
time.se = sd(boot.statistics)
time.se


me = ceiling(10 * 2 * time.se)/10
me


round(time.mean, 1) + c(-1, 1) * me
```

## Normal distribution, SD, SE

Note, if we do not use bootstraping, we can use the standard CI formula (https://www.mathsisfun.com/data/confidence-interval.html). This formula assumes normal distribution. As we can see, this is close to the result based on the bootstrapping method. 

$$\overline{X} \pm Z \frac{S}{\sqrt{n}}=29.11 \pm 1.96 \frac{20.72}{\sqrt{500}}=27.29, 30.93$$


Note that, in the following, the author used 2 times SE to calculate the CI. The relationship between SD and SE: 

"Now the sample mean will vary from sample to sample; the way this variation occurs is described by the “sampling distribution” of the mean. We can estimate how much sample means will vary from the standard deviation of this sampling distribution, which we call the standard error (SE) of the estimate of the mean. As the standard error is a type of standard deviation, confusion is understandable. Another way of considering the standard error is as a measure of the precision of the sample mean." (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1255808/)


```{R}
boot.mean = function(x,B,binwidth=NULL)
{
n = length(x)
boot.samples = matrix( sample(x,size=n*B,replace=TRUE), B, n)
boot.statistics = apply(boot.samples,1,mean)
se = sd(boot.statistics)
require(ggplot2)
if ( is.null(binwidth) )
binwidth = diff(range(boot.statistics))/30
p = ggplot(data.frame(x=boot.statistics),aes(x=x)) +
geom_histogram(aes(y=..density..),binwidth=binwidth) + geom_density(color="red")
plot(p)
interval = mean(x) + c(-1,1)*2*se
print( interval )
return( list(boot.statistics = boot.statistics, interval=interval, se=se, plot=p) )
}
```


```{r}
out = with(CommuteAtlanta, boot.mean(Distance, B = 1000))
```


## Sample function 

To understand the function of sample in R.

```{R}
sample(20,replace = TRUE)
```

The following uses loop to do the resampling. It uses sample function to index the numbers that they want to sample from the original sample. That is, [] suggests the indexing.

```{R}

n = length(CommuteAtlanta$Distance)
B = 1000
result = rep(NA, B)
for (i in 1:B)
{
boot.sample = sample(n, replace = TRUE)
result[i] = mean(CommuteAtlanta$Distance[boot.sample])
}

with(CommuteAtlanta, mean(Distance) + c(-1, 1) * 2 * sd(result))
```

## Proportion

So far, we have dealed with means. How about porpotions?Remember that, when calculating means, it starts with a single column of data to calculate the mean. Similarly, when calculating porpotions, you can just use a single column of data. 

```{R}
reeses = c(rep(1, 11), rep(0, 19))
reeses.boot = boot.mean(reeses, 1000, binwidth = 1/30)
```

However, if we have 48 students (i.e., 48 observations) and thus we have a bigger sample. However, how can we do re-sampling? Based on the note, it is kind of simple. They group them together and then resample from it. Note that, when they re-sampling, the programming do not distinguish the difference between 48 observations. But just combined them as a single column (741+699=1440), and then generate a very long column (1440 times 1000) and then reshape it into a matrix (1440 time 1000). This is the basic logic of the boot.mean function.

```{R}
reeses = c(rep(1, 741), rep(0, 699))
reeses.boot = boot.mean(reeses, 1000, binwidth = 0.005)
```

## boot package

After having a basic idea of boostrapping, we can then use the package of boot.

```{R}
library(boot)

data(CommuteAtlanta)

my.mean = function(x, indices)
{
return( mean( x[indices] ) )
}

time.boot = boot(CommuteAtlanta$Time, my.mean, 10000)

boot.ci(time.boot)
```

## Concept of Percentile

```{R}
require(Lock5Data)
data(ImmuneTea)
tea = with(ImmuneTea, InterferonGamma[Drink=="Tea"])
coffee = with(ImmuneTea, InterferonGamma[Drink=="Coffee"])
tea.mean = mean(tea)
coffee.mean = mean(coffee)
tea.n = length(tea)
coffee.n = length(coffee)



B = 500
# create empty arrays for the means of each sample
tea.boot = numeric(B)
coffee.boot = numeric(B)
# Use a for loop to take the samples
for ( i in 1:B )
  {
tea.boot[i] = mean(sample(tea,size=tea.n,replace=TRUE))
coffee.boot[i] = mean(sample(coffee,size=coffee.n,replace=TRUE))
}

boot.stat = tea.boot - coffee.boot
boot.stat

# Find endpoints for 90%, 95%, and 99% bootstrap confidence intervals using percentiles.

# 90%:  5% 95%
quantile(boot.stat,c(0.05,0.95))

# 95%: 2.5% 97.5%
quantile(boot.stat,c(0.025,0.975))

# 99%:  0.5% 99.5%
quantile(boot.stat,c(0.005,0.995))

```


## Bootstrapping for correlation interval

Some data and code are from: https://blog.methodsconsultants.com/posts/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/



```{R}
data_correlation<-read.csv("data_correlation.csv",fileEncoding="UTF-8-BOM")

data_correlation

```

```{R}
cor.test(data_correlation$LSAT,data_correlation$GPA)
```

In the following, I will write my own code to execute the bootstrapping. I set the bootstrapping number only 500, for illustrative purposes. As we can see, the distribution is not symmetrical. 

As we can see, the quantile result and c(-1, 1) X 2 are not the same, as the latter assumes symmetrical distribution. However, based on the histogram, we know it is not the case. Thus, quantile would be more appropriate. You can compare the result with that from the boot function.

```{R}
n_row = nrow(data_correlation)
n_row

set.seed(12345)

B = 500
result = rep(NA, B)
for (i in 1:B)
{
boot.sample = sample(n_row, replace = TRUE)
result_temp = cor.test(data_correlation[boot.sample,]$LSAT,data_correlation[boot.sample,]$GPA)
result[i]=result_temp$estimate
}
hist(result)

# 95%: 2.5% 97.5%
quantile(result,c(0.025,0.975))

sd(result)

mean(result) + c(-1, 1) * 1.96 * sd(result)

cor(data_correlation$LSAT,data_correlation$GPA)
cor(data_correlation$LSAT,data_correlation$GPA)+ c(-1, 1) * 1.96 * sd(result)


# why add 0.005? Not sure. The following is from the webpage. Later note: please refer to the webpage, as it provides the logic of basic interval.

0.776+0.005+c(-1, 1) * 1.96 * 0.131
```

In the blog mentioned above, the author used the boot function in R. For the logic of basic interval, please refer to:
https://blog.methodsconsultants.com/posts/understanding-bootstrap-confidence-interval-output-from-the-r-boot-package/


```{R}
library(boot)

get_r <- function(data, indices, x, y) {
  d <- data[indices, ]
  r <- round(as.numeric(cor(d[x], d[y])), 3)
  r}

set.seed(12345)

boot_out <- boot(
  data_correlation,
  x = "LSAT", 
  y = "GPA", 
  R = 500,
  statistic = get_r
)

boot.ci(boot_out)

```

# Poisson Regression

## Basic idea

The following is based on the lecture note of https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html

There is also some R code related to this.

https://rdrr.io/github/sta303-bolton/sta303w8/f/inst/rmarkdown/templates/philippines/skeleton/skeleton.Rmd

```{R}
data_HH <- read.csv("https://raw.githubusercontent.com/proback/BeyondMLR/master/data/fHH1.csv")

head(data_HH)
```

$$log (\lambda_X) =\beta_0+\beta_1 X$$
$$log (\lambda_{X+1}) =\beta_0+\beta_1 (X+1)$$

Thus,

$$log (\lambda_{X+1})-log (\lambda_X) =(\beta_0+\beta_1 (X+1))-(\beta_0+\beta_1 X)$$

Thus,

$$log (\frac{\lambda_{X+1}}{\lambda_X}) =\beta_1$$

Thus,

$$\frac{\lambda_{X+1}}{\lambda_X} =e^{\beta_1}$$

Note that, $\lambda$ here is the mean. It is poisson regression, and the parameter is the mean. Thus, $\frac{\lambda_{X+1}}{\lambda_X} =e^{\beta_1}$ suggests the ratio change in the DV as the IV change in one unit. 

$$log (\hat{\lambda}) =b_0+b_1 Age$$

```{R}
result_1 = glm(total ~ age, family = poisson, data = data_HH)
summary(result_1)
```

$$\frac{\lambda_{Age+1}}{\lambda_{Age}} =e^{\beta_1}=e^{-0.0047}=0.995$$


But, what does it mean? It is a bit tricky. But, we can make some modification to help us understand.

$$\lambda_{Age+1} =0.995 \lambda_{Age}$$
$$\lambda_{Age+1} - \lambda_{Age}=0.995 \lambda_{Age}- \lambda_{Age}=-0.005 \lambda_{Age}$$
Thus, we can understand that, the difference in the household size mean by changing 1 unit of age (i.e., $\lambda_{Age+1} - \lambda_{Age}$) is $-0.005 \lambda_{Age}$.

That is, the difference in the household size mean by changing 1 unit of age (i.e., $\lambda_{Age+1} - \lambda_{Age}$) is a decrease of 5% of $\lambda_{Age}$.


We can then calculate the confidence interval.

$$(\hat{\beta_1}-Z*SE(\hat{\beta_1}), \hat{\beta_1}+Z*SE(\hat{\beta_1}))$$

$$(-0.0047-1.96*0.00094,-0.0047+1.96*0.00094)=(−0.0065,−0.0029)$$

We can then plug them back to the exponential.


```{R}
exp(−0.0065)
exp(−0.0029)
```

$$(e^{−0.0065},e^{−0.0029})=(0.9935,0.9971)$$


You can also get the confidence interval directly use R code

```{R}
confint(result_1)
exp(confint(result_1))
```

Note that, we use original beta to construct a confidence interval and then exponentiate the endpoints is due to the fact that the oringal one is more close to normal distribution. 


## Trying to understand

With $\hat{\beta_0} = 1.55$ and $\hat{\beta_1}=-0.005$, we can write down the following. I also simulated the data and showed the relationship between X and Y. As we can see the figure, the relationship is pretty linear. Thus, something to keep in mind, the poisson distribution we typically see is the histogram of Y, rather than the relationship between X and Y. 

$$log(\hat{\lambda})=1.55-0.005 Age$$
$$\hat{\lambda}=e^{1.55-0.005 Age}$$

```{R}
data_age<-seq(10,100,0.5)
f_age<-function(x){exp(1.55-(0.005*x))}
cbind(data_age,f_age(data_age))
plot(data_age,f_age(data_age))
hist(f_age(data_age))
```

## Deviance

```{R}
basic_model <- glm(total ~ 1, family = poisson, data = data_HH)
deviance_1 <- anova(basic_model, result_1, test = "Chisq")
deviance_1

```

## Overdispersion (using another example)

```{R}
data_edu<-read.csv("https://raw.githubusercontent.com/proback/BeyondMLR/master/data/c_data.csv")
head(data_edu)
```

```{R}
hist(data_edu$nv)
```

```{R}
results_3<- glm(nv ~ type + region, family = poisson,
               offset = log(enroll1000), data = data_edu)
summary(results_3)
```

```{R}

results_4 <- glm(nv ~ type + region, family = quasipoisson,
               offset = log(enroll1000), data = data_edu)
summary(results_4)
difference_dev <- anova(results_4, results_3, test = "F")
difference_dev
```

# Use R for mediation

References:

https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html


https://advstats.psychstat.org/book/mediation/index.php


## Normal Distribution Case

The following code generates the data to be used in the mediation model. Based on the histogram, we can see that it follows Poisson distribution. 


```{R}
# Generate data for mediation analysis
# https://ademos.people.uic.edu/Chapter14.html

# Generate Possion data
# https://stats.stackexchange.com/questions/27443/generate-data-samples-from-poisson-regression

set.seed(123) 
N <- 200 
X <- rnorm(N, 1, 1)   
M <- 0.6*X + rnorm(N, 0, 1) 
mu <- exp(0.2*X+0.8*M ) 
Y <- rpois(n=N, lambda=mu)

Test_data <- data.frame(X, M, Y)

head(Test_data)
hist(Test_data$Y)

# write.csv(Test_data,"Test_data.csv")
```


Next, while in reality it follows Poisson distribution, the following assumes normal distribution. You will find the results are consistent with PROCESS.

```{R}

Normal_Mediation<-function(X, M, Y, data_used, resampling_size=5000)

  {

  result = rep(NA, resampling_size)
  n_row = nrow(data_used)
  
  for (i in 1:resampling_size)
  {
  boot.sample = sample(n_row, replace = TRUE)
  data_temp<-data_used[boot.sample,]
  
  # a path
  result_a_temp<-lm(M~X, data = data_temp)$coefficients
  names(result_a_temp) <- NULL
  a_0_temp<-result_a_temp[1]
  a_1_temp<-result_a_temp[2]
  
  # b path
  result_b_temp<-lm(Y~M+X, data = data_temp)$coefficients
  names(result_b_temp) <- NULL
  b_0_temp<-result_b_temp[1]
  b_1_temp<-result_b_temp[2]
  c_1_apostrophe_temp<-result_b_temp[3]
  
  #calculating the indirect effect
  indirect_temp<-a_1_temp*b_1_temp
  result[i]=indirect_temp
  }

  hist(result)
  sd(result)

  print(mean(result) + c(-1, 1) * 2 * sd(result))
   print(quantile(result,c(0.025,0.975)))

}

Normal_Mediation(X=X, M=M,Y=Y, data_used = Test_data,resampling_size=5000)

```

## Poisson Distribution Case

However, the problem is that the DV is count data. So, it is better to take that into consideration. The following is based on the paper of Geldhof 2017, Accommodating binary and count variables in mediation, A case for conditional indirect effects.

In particular,

Poisson regression uses the log link. For the b path function, it is as follows.

$$log(Y)=e^{b_0+b_1M+c^{'}X}$$

Thus, its first partial derivative again M is as follows.

$$b_1e^{b_0+b_1M+c^{'}X}$$
Where,

$$M=a_0+a_1X$$

Thus, the indirect effect is as follows.

$$IndirectEffect = a_1b_1e^{b_0+b_1M+c^{'}X}=a_1b_1e^{b_0+b_1(a_0+a_1X)+c^{'}X}$$


As we can see the indirect effect is not a constant, as it depends on X. Different X values will lead to different indirect effects. Thus, you can see the following R code takes this into consideration. 

```{R}

# x_predetermined = 0 : X = Mean
# x_predetermined = 1 : X = Mean + SD
# x_predetermined = -1 : X = Mean - SD

Poisson_Mediation<-function(X, M, Y, data_used, x_predetermined=0, resampling_size=5000) 
{
  
    result = rep(NA, resampling_size)
    n_row = nrow(data_used)
    
    
    if(x_predetermined==0){x_predetermined=mean(data_used$X)}
    else if (x_predetermined==-1){x_predetermined=mean(data_used$X)-sd(data_used$X)}
    else(x_predetermined=mean(data_used$X)+sd(data_used$X))

for (i in 1:resampling_size)
{
  boot.sample = sample(n_row, replace = TRUE)
  data_temp<-data_used[boot.sample,]
  
  # a path
  result_a_temp<-lm(M~X, data = data_temp)$coefficients
  names(result_a_temp) <- NULL
  a_0_temp<-result_a_temp[1]
  a_1_temp<-result_a_temp[2]
  
  # b path
  result_b_temp<-glm(Y~M+X, data = data_temp, family = quasipoisson)$coefficients
  names(result_b_temp) <- NULL
  b_0_temp<-result_b_temp[1]
  b_1_temp<-result_b_temp[2]
  c_1_apostrophe_temp<-result_b_temp[3]
  
  #calculating the indirect effect
  
  M_estimated_temp=a_0_temp+a_1_temp*x_predetermined
  indirect_temp<-a_1_temp*b_1_temp*exp(b_0_temp+b_1_temp*M_estimated_temp+c_1_apostrophe_temp*x_predetermined)
  result[i]=indirect_temp
}
hist(result)
quantile(result,c(0.025,0.975))
}

# X = Mean
Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=0,resampling_size=5000)

# X = Mean - 1 SD
Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=-1,resampling_size=5000)

# X = Mean + 1 SD
Poisson_Mediation(X=X, M=M,Y=Y, data_used = Test_data,x_predetermined=1,resampling_size=5000)

```
